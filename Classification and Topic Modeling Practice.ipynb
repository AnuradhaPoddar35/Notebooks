{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Arxiv Data\n",
    "\n",
    "We're going to use a bunch of Arxiv physics papers for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're already in the directory with the papers, so we can use os.listdir() to get the file names\n",
    "filename_list = os.listdir()[0:1000]\n",
    "\n",
    "# There are a few extraneous files we don't want to get caught in the \n",
    "# filenamelist, so we add some logic to exclude those\n",
    "filename_list = [filename for filename in filename_list if len(filename) < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0301116', '0304232', '0303017', '0303225', '0302131']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that these file names are correct:\n",
    "filename_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in all the files from the `filename_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(filename_list)):\n",
    "    \n",
    "    filename = filename_list[i]\n",
    "    \n",
    "    # errors='ignore' is added to deal with UnicodeDecodeErrors  \n",
    "    with open(filename, 'r', errors='ignore') as file:\n",
    "            file_contents = file.read()\n",
    "          \n",
    "    # Add document to corpus\n",
    "    corpus.append(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing LaTeX and other formatting artifacts that will cause issues with NMF and LDA\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import gensim.parsing.preprocessing as genpre\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def prep_text(text):\n",
    "     # this removes LaTeX formatting, citations, splits hyphens\n",
    "    myreg = r'\\\\[\\w]+[\\{| ]|\\$[^\\$]+\\$|\\(.+\\, *\\d{2,4}\\w*\\)|\\S*\\/\\/\\S*|[\\\\.,\\/#!$%\\^&\\*;:{}=_`\\'\\\"~()><\\|]|\\[.+\\]|\\d+|\\b\\w{1,2}\\b'\n",
    "    parsed_data = text.replace('-', ' ')\n",
    "    parsed_data = re.sub(myreg, '', parsed_data)\n",
    "    parsed_data = [lmtzr.lemmatize(w) for w in parsed_data.lower().split() if w not in genpre.STOPWORDS]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [prep_text(document) for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prep_text` didn't remove -everything-, but we will have many fewer artifacts than if we didn't run it at all. We can also scrape off some very common LaTeX phrases by passing them as stopwords when retraining the `TfIdfVectorizer`, and also by setting `max_df` to exclude words that occur in more than 90% of documents.\n",
    "\n",
    "See this [excellent blog post](https://medium.com/@omar.abdelbadie1/processing-text-for-topic-modeling-c355e907ab23) on why `prep_text` works to remove LaTeX artifacts. All credit goes to author Omar Abdelbadie for this method.\n",
    "\n",
    "Note that by using `prep_text` we've caused every entry in `corpus` to become a list containing a number of strings, rather than one big string for each entry. This is a problem for when we want to create our feature matrix, as `TfIdfVectorizer` is not compatible with a list of lists. We'll need to use `join` (a string method) to change each entry back to a string instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = ' '.join(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Feature Matrix\n",
    "\n",
    "Now we have 997 documents to work with. Now we can turn our corpus into a matrix of Term Frequency Inverse Document Frequency (TF-IDF) features using `sklearn`'s `TfidfVectorizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<997x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 908182 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again ignoring any UnicodeDecodeErrors\n",
    "vectorizer = TfidfVectorizer(decode_error = 'ignore', max_df = 0.9, ngram_range = (1, 2), max_features = 20000)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the vocabulary that was learned by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latex': 10101,\n",
       " 'file': 6930,\n",
       " 'paper': 12957,\n",
       " 'documentstylerevtex': 4967,\n",
       " 'documentstylearticle': 4965,\n",
       " 'beqequation': 1288,\n",
       " 'eeqequation': 5273,\n",
       " 'beqaeqnarray': 1286,\n",
       " 'eeqaeqnarray': 5272,\n",
       " 'partial': 13043,\n",
       " 'lraleftrightarrow': 10768,\n",
       " 'footnote': 7205,\n",
       " 'thefootnotefootnote': 18056,\n",
       " 'theequationsectionequation': 18051,\n",
       " 'defnonumber': 4122,\n",
       " 'deffootnote': 4044,\n",
       " 'def': 3970,\n",
       " 'original': 12836,\n",
       " 'draft': 5042,\n",
       " 'flushright': 7079}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the vocab dict to a list so we can print just a subset of the dict\n",
    "\n",
    "first20_vocab = {k: vectorizer.vocabulary_[k] for k in list(vectorizer.vocabulary_)[:20]}\n",
    "first20_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 guaranteed killing\n",
      "1 eqnarray jdef\n",
      "2 follows conventional\n",
      "3 embigl circcal\n",
      "4 broomstick hand\n",
      "5 sphere derived\n",
      "6 eqn nipi\n",
      "7 member elementary\n",
      "8 ovl\n",
      "9 otto stark\n",
      "10 group prove\n",
      "11 corresp\n",
      "12 left coshlambdaright\n",
      "13 altscen\n",
      "14 zero leftdel\n",
      "15 topological case\n",
      "16 manifold providing\n",
      "17 constraint possible\n",
      "18 leftprime\n",
      "19 ncond operator\n"
     ]
    }
   ],
   "source": [
    "# Similar to above, use itertools to avoid printing the entire (massive) set to screen\n",
    "import itertools\n",
    "\n",
    "for i, val in enumerate(itertools.islice(vectorizer.stop_words_, 20)):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of LaTeX and nonsense terms, as well as some physics terms, were caught in the filter created by `max_df`. The benefit should outweigh the cost of excluding these particular physics terms. (After all, they must not be very distinctive phrases if they're occurring in 90% of papers.)\n",
    "\n",
    "The top of the vocabulary showed some additional phrases that occur frequently and are not informative to us. We'll go ahead and remove those low-hanging fruit using by retraining the vectorizer and passing these stop-phrases as a list.\n",
    "\n",
    "(Normally it would not make sense to slice a dictionary this way, but after having run the vectorizer repeatedly and seeing the order the terms are stored in memory, we can make the call to exclude the 15 phrases that tend to float to the top.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = list(vectorizer.vocabulary_)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<997x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 902123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', max_df=0.9, ngram_range=(1, 2), \n",
    "                             max_features = 20000, stop_words=additional_stopwords)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some topic modeling.\n",
    "\n",
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NMF\n",
    "nmf_model = NMF(n_components = 10, solver = 'mu')\n",
    "\n",
    "# Create variable to make it easy to retrieve topics\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=None, shuffle=False, solver='mu',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_components = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: operator, symmetry, case, form, function, model, space, term, gauge, phys\n",
      "Topic 2: eeq, string, cosmological, citation hep, citation, hep, solution, bulk, branes, brane\n",
      "Topic 3: metric, cosmological, schwarzschild, solution, sitter, entropy, horizon, hole, black hole, black\n",
      "Topic 4: array, right equation, right right, eqnarray, equation left, left left, right left, left right, right, left\n",
      "Topic 5: lett citation, jhep, jhep citation, arxivhep citation, arxivhep, phys citation, hep citation, hep, citation hep, citation\n",
      "Topic 6: string theory, light cone, cone, plane, wave background, state, plane wave, background, wave, string\n",
      "Topic 7: closed string, energy, sen, rolling tachyon, potential, kink, rolling, inflation, string, tachyon\n",
      "Topic 8: ghost, gauge, action, delta, phi, nonumber, big, gamma, eqnarray eqnarray, eqnarray\n",
      "Topic 9: operator, product, star, noncommutative space, quantum, space, noncommutativity, commutative, algebra, noncommutative\n",
      "Topic 10: vacuum, curve, lref, modulus, gauge theory, model, matrix, gauge, superpotential, matrix model\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_components):\n",
    "    print(\"Topic {}: {}\".format(i + 1, \", \".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exciting! We have a few topics here that are composed of LaTeX specifications, but the others are clearly relevant to particular areas of physics.\n",
    "\n",
    "Let's try out some LDA as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: equation phys, explicitly equation, eq, inherent, note particular, zi zi, brane model, wm, ortiz, tba\n",
      "Topic 2: string, constant, citation, matrix model, practical, black hole, sphere, align, gauge, lref\n",
      "Topic 3: angle decorsize, plainleft, distmm, distmm angle, decorsize, fmfgraph, label distmm, decorsize label, wh wh, wh\n",
      "Topic 4: branes, state, citation hep, right, left, citation, hep, boundary, brane, string\n",
      "Topic 5: left, fixed equation, eea, kato, big, difficult, gamma, psi, eqnarray, string\n",
      "Topic 6: array, citation, right, left, model, operator, phys, gauge, string, eqnarray\n",
      "Topic 7: sphere, tau, cubic string, bare parameter, fuzzy, physrev, reuter, kac, path path, fuzzy sphere\n",
      "Topic 8: charge, energy, term, tachyon, branes, action, left, right, wall, brane\n",
      "Topic 9: ext, form, interaction, gauge, color, mechanic, eqnarray, right, left, phys\n",
      "Topic 10: solution, citation, gauge, right, brane, hep, left, eqnarray, string, phys\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic {}: {}\".format(i + 1, \", \".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the topics generated by LDA are not very interesting or distinct. It looks like NMF is a more appropriate topic modeling method for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "Now we're going to do some clustering. In order to get an appropriate matrix, we'll use NMF's `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       3, 0, 0, 1, 2, 3, 1, 2, 3, 1, 1, 1, 3, 1, 1, 2, 3, 2, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       3, 3, 2, 1, 2, 3, 1, 1, 2, 0, 1, 1, 0, 2, 2, 1, 3, 1, 3, 3, 1, 2,\n",
       "       1, 1, 1, 3, 1, 2, 1, 2, 3, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 3, 1, 0,\n",
       "       1, 1, 3, 1, 1, 1, 1, 3, 0, 3, 1, 2, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 2, 1,\n",
       "       2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 3, 2, 1, 1, 1, 1, 2, 1,\n",
       "       2, 1, 1, 3, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3,\n",
       "       1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1,\n",
       "       3, 0, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 0, 1, 3, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 0, 1, 1, 2, 3, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1,\n",
       "       2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3,\n",
       "       3, 2, 1, 1, 1, 1, 1, 0, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 3, 0, 3, 0, 2,\n",
       "       3, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 0, 0, 1,\n",
       "       1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "       2, 1, 1, 0, 1, 1, 1, 1, 2, 3, 1, 0, 1, 2, 1, 3, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 3, 1, 1, 3, 2, 1, 1, 1, 2, 1, 1, 2, 0, 2, 3, 2, 3, 2, 1, 2,\n",
       "       2, 2, 3, 1, 1, 1, 1, 2, 1, 3, 0, 3, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1,\n",
       "       1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 3, 0, 2, 1, 1, 1, 3, 1, 3,\n",
       "       1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 2, 1, 3,\n",
       "       1, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, 1, 3, 0, 1, 0, 0, 2, 3,\n",
       "       3, 0, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n",
       "       1, 2, 2, 1, 1, 1, 0, 1, 3, 1, 1, 2, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 2, 2, 3, 1, 2, 1, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "       1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1,\n",
       "       1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 0, 1, 3, 1, 1, 1, 3, 1, 1, 0, 0, 1,\n",
       "       3, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 3, 1, 3, 3,\n",
       "       1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 3, 1, 3, 1,\n",
       "       1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 3, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 3, 1, 2, 1, 0, 1, 3, 1, 1, 3,\n",
       "       0, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 3,\n",
       "       1, 1, 1, 1, 3, 1, 3, 2, 3, 3, 2, 1, 1, 0, 1, 2, 0, 1, 2, 0, 2, 1,\n",
       "       1, 1, 2, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1,\n",
       "       1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1,\n",
       "       1, 2, 2, 1, 1, 3, 3, 1, 0, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 3,\n",
       "       3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2,\n",
       "       1, 2, 1, 2, 1, 3, 1, 1, 0, 3, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 3, 3, 1, 3], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = nmf_model.fit_transform(X)\n",
    "\n",
    "# First try with 4\n",
    "kmeans4 = KMeans(n_clusters=4, random_state=6).fit(X2)\n",
    "kmeans4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, 8 clusters\n",
    "kmeans8 = KMeans(n_clusters=8, random_state=6).fit(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, 12\n",
    "kmeans12 = KMeans(n_clusters=12, random_state=6).fit(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06046452, 0.0060843 , 0.00682197, 0.00705394, 0.01819956,\n",
       "        0.02579614, 0.00946373, 0.00929777, 0.0187972 , 0.01835133],\n",
       "       [0.02521794, 0.0122218 , 0.19401777, 0.00199063, 0.02122031,\n",
       "        0.01910719, 0.00594836, 0.01326531, 0.00317477, 0.00561036],\n",
       "       [0.04134829, 0.12306222, 0.01429934, 0.01457846, 0.01819093,\n",
       "        0.0218589 , 0.06239315, 0.01193312, 0.00334262, 0.02014902],\n",
       "       [0.04489066, 0.00243255, 0.00605031, 0.09647825, 0.01189382,\n",
       "        0.01641603, 0.00836684, 0.09700811, 0.02856672, 0.01375213]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans4.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the metrics to see which number of clusters is most ideal for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 4, 'n_init': 10, 'n_jobs': None, 'precompute_distances': 'auto', 'random_state': 6, 'tol': 0.0001, 'verbose': 0} clusters: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-27bc2e7b20ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkmeans4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-27bc2e7b20ab>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(km)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Metrics for %s clusters: \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_clusters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Homogeneity: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhomogeneity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completeness: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompleteness_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"V-measure: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_measure_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "def get_metrics(km):\n",
    "    print(\"Metrics for %s clusters: \" % km.get_params('n_clusters'))\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "    print(\"Adjusted Rand-Index: %.3f\"\n",
    "          % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "    \n",
    "for model in [kmeans4, kmeans8, kmeans12]:\n",
    "    get_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-15628b41d055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_hashing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top terms per cluster:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moriginal_space_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opts' is not defined"
     ]
    }
   ],
   "source": [
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: aa abc abdus abdus salam abbreviation abcd abdalla ab aai aaa\n",
      "Cluster 1: aai aa abbreviation abc abdalla aaa abcd abdus salam abdus ab\n",
      "Cluster 2: aaa abcd aa abc abdus salam abbreviation ab aai abdalla abdus\n",
      "Cluster 3: abdalla ab aa abdus abc abdus salam abbreviation abcd aai aaa\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmeans4.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(4):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
