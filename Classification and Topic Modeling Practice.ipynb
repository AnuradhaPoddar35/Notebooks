{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Arxiv Data\n",
    "\n",
    "We're going to use a bunch of Arxiv physics papers for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're already in the directory with the papers, so we can use os.listdir() to get the file names\n",
    "filename_list = os.listdir()[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0301116', '0304232', '0303017', '0303225', '0302131']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that these file names are correct:\n",
    "filename_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in all the files from the `filename_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(filename_list)):\n",
    "    \n",
    "    filename = filename_list[i]\n",
    "    \n",
    "    # errors='ignore' is added to deal with UnicodeDecodeErrors  \n",
    "    with open(filename, 'r', errors='ignore') as file:\n",
    "            file_contents = file.read()\n",
    "          \n",
    "    # Add document to corpus\n",
    "    corpus.append(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing LaTeX and other formatting artifacts that will cause issues with NMF and LDA\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import gensim.parsing.preprocessing as genpre\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def prep_text(text):\n",
    "     # this removes LaTeX formatting, citations, splits hyphens\n",
    "    myreg = r'\\\\[\\w]+[\\{| ]|\\$[^\\$]+\\$|\\(.+\\, *\\d{2,4}\\w*\\)|\\S*\\/\\/\\S*|[\\\\.,\\/#!$%\\^&\\*;:{}=_`\\'\\\"~()><\\|]|\\[.+\\]|\\d+|\\b\\w{1,2}\\b'\n",
    "    parsed_data = text.replace('-', ' ')\n",
    "    parsed_data = re.sub(myreg, '', parsed_data)\n",
    "    parsed_data = [lmtzr.lemmatize(w) for w in parsed_data.lower().split() if w not in genpre.STOPWORDS]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [prep_text(document) for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prep_text` didn't remove -everything-, but we will have many fewer artifacts than if we didn't run it at all. We can also scrape off some very common LaTeX phrases by passing them as stopwords when retraining the `TfIdfVectorizer`, and also by setting `max_df` to exclude words that occur in more than 90% of documents.\n",
    "\n",
    "See this [excellent blog post](https://medium.com/@omar.abdelbadie1/processing-text-for-topic-modeling-c355e907ab23) on why `prep_text` works to remove LaTeX artifacts. All credit goes to author Omar Abdelbadie for this method.\n",
    "\n",
    "Note that by using `prep_text` we've caused every entry in `corpus` to become a list containing a number of strings, rather than one big string for each entry. This is a problem for when we want to create our feature matrix, as `TfIdfVectorizer` is not compatible with a list of lists. We'll need to use `join` (a string method) to change each entry back to a string instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = ' '.join(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Feature Matrix\n",
    "\n",
    "We have exactly 500 documents to work with. Now we can turn our corpus into a matrix of Term Frequency Inverse Document Frequency (TF-IDF) features using `sklearn`'s `TfidfVectorizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 203169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again ignoring any UnicodeDecodeErrors\n",
    "vectorizer = TfidfVectorizer(decode_error = 'ignore', max_df = 0.9, ngram_range = (2, 2), max_features = 20000)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the vocabulary that was learned by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latex file': 9941,\n",
       " 'beqequation eeqequation': 1152,\n",
       " 'beqaeqnarray eeqaeqnarray': 1150,\n",
       " 'footnote thefootnotefootnote': 7334,\n",
       " 'document draft': 4858,\n",
       " 'particle physic': 13017,\n",
       " 'physic department': 13265,\n",
       " 'department physic': 4202,\n",
       " 'new york': 11861,\n",
       " 'university new': 19151,\n",
       " 'today maketitle': 18727,\n",
       " 'maketitle abstract': 10736,\n",
       " 'abstract consider': 45,\n",
       " 'finite thickness': 7109,\n",
       " 'kinetic term': 9729,\n",
       " 'bulk scalar': 1843,\n",
       " 'vacuum expectation': 19264,\n",
       " 'expectation value': 6366,\n",
       " 'value coupling': 19316,\n",
       " 'kaluza klein': 9651}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the vocab dict to a list so we can print just a subset of the dict\n",
    "\n",
    "first20_vocab = {k: vectorizer.vocabulary_[k] for k in list(vectorizer.vocabulary_)[:20]}\n",
    "first20_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 word exactly\n",
      "1 non lymext\n",
      "2 entropy like\n",
      "3 baye phys\n",
      "4 main difficulty\n",
      "5 covariant quantized\n",
      "6 covariant admits\n",
      "7 give displayed\n",
      "8 dltkphi unlike\n",
      "9 remember non\n",
      "10 specifying exact\n",
      "11 order green\n",
      "12 hole predicts\n",
      "13 lqcmartin bojowaldisotropic\n",
      "14 start description\n",
      "15 let separate\n",
      "16 bigvevbigllanglebigrrangle bigcomm\n",
      "17 seiberg gauged\n",
      "18 chosen phase\n",
      "19 superfield read\n"
     ]
    }
   ],
   "source": [
    "# Similar to above, use itertools to avoid printing the entire (massive) set to screen\n",
    "import itertools\n",
    "\n",
    "for i, val in enumerate(itertools.islice(vectorizer.stop_words_, 20)):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of LaTeX and nonsense terms, as well as some physics terms, were caught in the filter created by `max_df`. The benefit should outweigh the cost of excluding these particular physics terms. (After all, they must not be very distinctive phrases if they're occurring in 90% of papers.)\n",
    "\n",
    "The top of the vocabulary showed some additional phrases that occur frequently and are not informative to us. We'll go ahead and remove those low-hanging fruit using by retraining the vectorizer and passing these stop-phrases as a list.\n",
    "\n",
    "(Normally it would not make sense to slice a dictionary this way, but after having run the vectorizer repeatedly and seeing the order the terms are stored in memory, we can make the call to exclude the 15 phrases that tend to float to the top.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = list(vectorizer.vocabulary_)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 203169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', max_df=0.9, ngram_range=(2, 2), \n",
    "                             max_features = 20000, stop_words=additional_stopwords)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some topic modeling.\n",
    "\n",
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NMF\n",
    "nmf_model = NMF(n_components = 10, solver = 'mu')\n",
    "\n",
    "# Create variable to make it easy to retrieve topics\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=None, shuffle=False, solver='mu',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_components = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: def def, rev citation, jhep citation, arxivhep citation, gauge theory, lett citation, nucl phys, phys citation, hep citation, citation hep\n",
      "Topic 2: defrelax defem, lie group, lie algebra, moody algebra, kac moody, defrelax defrelax, defssbchar defssbchar, em em, defem defem, def def\n",
      "Topic 3: einstein equation, momentum tensor, energy momentum, scale factor, cosmic string, extra dimension, energy density, cosmological constant, phys rev, scalar field\n",
      "Topic 4: sitter space, reissner nordstr, quasinormal mode, near horizon, quasinormal frequency, hole entropy, phys rev, quantum gravity, schwarzschild black, black hole\n",
      "Topic 5: universal solution, non bps, rolling tachyon, string theory, tachyon condensation, field theory, boundary state, open string, string field, closed string\n",
      "Topic 6: bmn operator, zero mode, maximally supersymmetric, gamma gamma, type iib, cone gauge, penrose limit, light cone, wave background, plane wave\n",
      "Topic 7: riemann surface, witten curve, effective superpotential, dijkgraaf vafa, seiberg witten, supersymmetric gauge, modulus space, fuzzy sphere, gauge theory, matrix model\n",
      "Topic 8: right nonumber, eq left, bf equation, nonumber left, right equation, equation left, right right, left left, right left, left right\n",
      "Topic 9: phys lett, yang mill, lie algebra, field theory, phase space, space time, gauge theory, nucl phys, star product, hilbert space\n",
      "Topic 10: equation alpha, given equation, affine algebra, equation partial, right equation, array array, equation left, left array, array right, equation equation\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_components):\n",
    "    print(\"Topic {}: {}\".format(i + 1, \", \".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exciting! We have a few topics here that are composed of LaTeX specifications (like 1, 2, 8, and 10), but the others are clearly relevant to particular areas of physics.\n",
    "\n",
    "Let's try out some LDA as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: bethe ansatz, field theory, left right, trivial galois, math phys, form factor, lambda lambda, right left, galois current, om om\n",
      "Topic 2: schwarzschild black, left right, theequationsectionequationlyter eqnarray, alice string, brick wall, equation lyter, phys rev, eqnarray lyter, def def, black hole\n",
      "Topic 3: hep citation, black hole, gauge theory, left right, equation equation, nucl phys, phys rev, field theory, citation hep, def def\n",
      "Topic 4: displaymath displaymath, decorsize label, reference thebibliography, phin phin, eigenvalue problem, hilbert space, function invariant, inner product, psin phin, wh wh\n",
      "Topic 5: scattering amplitude, cosmological horizon, internal space, black hole, left right, eq eeq, phys rev, sitter space, equation equation, photon photon\n",
      "Topic 6: spacelike infinity, gauge theory, quantum grav, little group, gauge transformation, citation hep, van proeyen, theta theta, physic publishing, preprint hep\n",
      "Topic 7: alpha alpha, random force, wilson line, gauge group, planck mass, citation hep, dissipative system, extended planck, def def, inflaton field\n",
      "Topic 8: anomalous dimension, second class, class constraint, gamma gamma, complex function, state family, lk lk, algebra object, tau tau, rput rput\n",
      "Topic 9: string field, ki zi, zi zi, fokker planck, field theory, phys rev, langevin eq, citation hep, plane wave, langevin equation\n",
      "Topic 10: coupling expansion, hline hline, strong coupling, hopf algebra, summable series, critical exponent, anharmonic oscillator, non borel, borel summable, variational perturbation\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic {}: {}\".format(i + 1, \", \".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the topics generated by LDA are not very interesting or distinct. It looks like NMF is a more appropriate topic modeling method for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
